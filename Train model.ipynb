{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract labels and image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1915, 1)\n",
      "(507, 1)\n",
      "(1002, 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert the label from .csv files to numpy array\n",
    "# for train label\n",
    "train_label = pd.read_csv(r'') #Path for training label.csv file\n",
    "label_list = train_label.values.tolist()\n",
    "for j in label_list: \n",
    "    del j[0]\n",
    "label_list = np.array(label_list)\n",
    "print(label_list.shape)\n",
    "\n",
    "# for validation label\n",
    "validation_label = pd.read_csv(r'') #Path for validation_label.csv file\n",
    "valid_label_list = validation_label.values.tolist()\n",
    "for k in valid_label_list: \n",
    "    del k[0]\n",
    "valid_label_list = np.array(valid_label_list)\n",
    "print(valid_label_list.shape)\n",
    "\n",
    "# for validation label\n",
    "test_label = pd.read_csv(r'') #Path for test_label.csv file\n",
    "test_label_list = test_label.values.tolist()\n",
    "for k in test_label_list: \n",
    "    del k[0]\n",
    "test_label_list = np.array(test_label_list)\n",
    "print(test_label_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and process the images from train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1915, 32, 32, 3)\n",
      "1915\n"
     ]
    }
   ],
   "source": [
    "# load the image from train data\n",
    "img_dir = r\"\" #Image path for cropped and resized images\n",
    "data_path = os.path.join(img_dir,'*g') \n",
    "files = glob.glob(data_path) \n",
    "\n",
    "img_array = []\n",
    "\n",
    "for i in files:\n",
    "    img = load_img(i)\n",
    "    img_array.append(img_to_array(img))\n",
    "    \n",
    "img_array = np.array(img_array)    \n",
    "print(img_array.shape)\n",
    "# Normalize data in the numppy array\n",
    "img_array = img_array/255.0\n",
    "print(len(img_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and process the images from validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(507, 32, 32, 3)\n",
      "507\n"
     ]
    }
   ],
   "source": [
    "# load the image from validation data\n",
    "v_img_dir = r\"\" #Image path for validation cropped and resized images\n",
    "v_data_path = os.path.join(v_img_dir,'*g') \n",
    "v_files = glob.glob(v_data_path) \n",
    "\n",
    "v_img_array = []\n",
    "\n",
    "for o in v_files:\n",
    "    v_img = load_img(o)\n",
    "    v_img_array.append(img_to_array(v_img))\n",
    "    \n",
    "v_img_array = np.array(v_img_array)    \n",
    "\n",
    "print(v_img_array.shape)\n",
    "# Normalize data in the numppy array\n",
    "v_img_array = v_img_array/255.0\n",
    "print(len(v_img_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and process the images from test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "(1002, 32, 32, 3)\n",
      "1002\n"
     ]
    }
   ],
   "source": [
    "# load the image from test data\n",
    "t_img_dir = r\"\" #Image path for test cropped and resized images\n",
    "t_data_path = os.path.join(t_img_dir,'*g') \n",
    "t_files = glob.glob(t_data_path) \n",
    "\n",
    "t_img_array = []\n",
    "tcount = 0\n",
    "\n",
    "for p in t_files:\n",
    "    t_img = load_img(p)\n",
    "    t_img_array.append(img_to_array(t_img))\n",
    "    tcount += 1\n",
    "t_img_array = np.array(t_img_array)    \n",
    "print(tcount)\n",
    "print(t_img_array.shape)\n",
    "# Normalize data in the numppy array\n",
    "t_img_array = t_img_array/255.0\n",
    "print(len(t_img_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 30, 30, 8)         224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 15, 15, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 13, 13, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 5, 5, 16)          1040      \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 5, 5, 16)          272       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                6416      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 9,426\n",
      "Trainable params: 9,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Design a sequential CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(8, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (2, 2), activation='relu'))\n",
    "model.add(layers.Conv2D(16, (1, 1), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='softmax'))\n",
    "model.add(layers.Dense(2))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with 'SGD' optimizer and sparse_categorical_crossentropy loss function\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6038 - accuracy: 0.6961 - val_loss: 1.1959 - val_accuracy: 0.0907\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6024 - accuracy: 0.6961 - val_loss: 1.1909 - val_accuracy: 0.0907\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6008 - accuracy: 0.6961 - val_loss: 1.2393 - val_accuracy: 0.0907\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5990 - accuracy: 0.6961 - val_loss: 1.2415 - val_accuracy: 0.0907\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5967 - accuracy: 0.6961 - val_loss: 1.2871 - val_accuracy: 0.0907\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5950 - accuracy: 0.6961 - val_loss: 1.3464 - val_accuracy: 0.0907\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5927 - accuracy: 0.6961 - val_loss: 1.3073 - val_accuracy: 0.0907\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5899 - accuracy: 0.6961 - val_loss: 1.4073 - val_accuracy: 0.0907\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5874 - accuracy: 0.6961 - val_loss: 1.4600 - val_accuracy: 0.0907\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5849 - accuracy: 0.6961 - val_loss: 1.4623 - val_accuracy: 0.0907\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 0.5838 - accuracy: 0.6961 - val_loss: 1.6105 - val_accuracy: 0.0907\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5821 - accuracy: 0.6961 - val_loss: 1.6352 - val_accuracy: 0.0907\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.5811 - accuracy: 0.6961 - val_loss: 1.5483 - val_accuracy: 0.0907\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.5813 - accuracy: 0.6961 - val_loss: 1.5422 - val_accuracy: 0.0907\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5798 - accuracy: 0.6961 - val_loss: 1.6298 - val_accuracy: 0.0907\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5783 - accuracy: 0.6961 - val_loss: 1.4144 - val_accuracy: 0.0907\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5792 - accuracy: 0.6961 - val_loss: 1.7561 - val_accuracy: 0.0907\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5785 - accuracy: 0.6961 - val_loss: 1.7289 - val_accuracy: 0.0907\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5787 - accuracy: 0.6961 - val_loss: 1.5993 - val_accuracy: 0.0907\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5763 - accuracy: 0.6961 - val_loss: 1.5247 - val_accuracy: 0.0907\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5787 - accuracy: 0.6961 - val_loss: 1.5092 - val_accuracy: 0.0907\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5786 - accuracy: 0.6961 - val_loss: 1.8060 - val_accuracy: 0.0907\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5769 - accuracy: 0.6961 - val_loss: 1.8378 - val_accuracy: 0.0907\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5783 - accuracy: 0.6961 - val_loss: 1.7782 - val_accuracy: 0.0907\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5771 - accuracy: 0.6961 - val_loss: 1.5987 - val_accuracy: 0.0907\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5764 - accuracy: 0.6961 - val_loss: 1.6456 - val_accuracy: 0.0907\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5768 - accuracy: 0.6961 - val_loss: 1.7274 - val_accuracy: 0.0907\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5746 - accuracy: 0.6961 - val_loss: 1.5507 - val_accuracy: 0.0907\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5753 - accuracy: 0.6961 - val_loss: 1.9548 - val_accuracy: 0.0907\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5755 - accuracy: 0.6961 - val_loss: 1.8766 - val_accuracy: 0.0907\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5768 - accuracy: 0.6961 - val_loss: 1.7521 - val_accuracy: 0.0907\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5773 - accuracy: 0.6961 - val_loss: 1.7162 - val_accuracy: 0.0907\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5772 - accuracy: 0.6961 - val_loss: 1.7325 - val_accuracy: 0.0907\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5751 - accuracy: 0.6961 - val_loss: 1.9802 - val_accuracy: 0.0907\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5757 - accuracy: 0.6961 - val_loss: 1.6973 - val_accuracy: 0.0907\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5756 - accuracy: 0.6961 - val_loss: 1.4702 - val_accuracy: 0.0907\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5760 - accuracy: 0.6961 - val_loss: 1.5809 - val_accuracy: 0.0907\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5761 - accuracy: 0.6961 - val_loss: 1.8245 - val_accuracy: 0.0907\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5754 - accuracy: 0.6961 - val_loss: 1.8871 - val_accuracy: 0.0907\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5757 - accuracy: 0.6961 - val_loss: 1.9044 - val_accuracy: 0.0907\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5751 - accuracy: 0.6961 - val_loss: 1.6821 - val_accuracy: 0.0907\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5750 - accuracy: 0.6961 - val_loss: 1.9039 - val_accuracy: 0.0907\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5754 - accuracy: 0.6961 - val_loss: 1.7054 - val_accuracy: 0.0907\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 0.5747 - accuracy: 0.6961 - val_loss: 1.8894 - val_accuracy: 0.0907\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5751 - accuracy: 0.6961 - val_loss: 1.7079 - val_accuracy: 0.0907\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5753 - accuracy: 0.6961 - val_loss: 1.9496 - val_accuracy: 0.0907\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5760 - accuracy: 0.6961 - val_loss: 1.8832 - val_accuracy: 0.0907\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5730 - accuracy: 0.6961 - val_loss: 2.1683 - val_accuracy: 0.0907\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5738 - accuracy: 0.6961 - val_loss: 2.1668 - val_accuracy: 0.0907\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5761 - accuracy: 0.6961 - val_loss: 1.9181 - val_accuracy: 0.0907\n",
      "WARNING:tensorflow:From C:\\Users\\tejug\\Conda\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\tejug\\Conda\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\tejug\\OneDrive\\Documents\\Machine Learning\\Project\\Code\\final code\\assets\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(img_array, label_list, epochs=50, validation_data=(v_img_array, valid_label_list))\n",
    "model.save(r'C:\\Code\\final code') # Path to save the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 - 0s - loss: 1.9191 - accuracy: 0.0938\n",
      "Test Loss : 1.9191396236419678 and test accuracy : 0.09381237626075745\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1.25])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(t_img_array,  test_label_list, verbose=2)\n",
    "print(\"Test Loss : \" + str(test_loss) + \" and test accuracy : \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
